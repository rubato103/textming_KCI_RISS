# 한국어 형태소 분석 프롬프트 가이드

## 📋 목차

0. [AI 프롬프트 작성 원칙](#ai-프롬프트-작성-원칙)
1. [환경 셋팅하기](#1-환경-셋팅하기)
2. [데이터 불러오기](#2-데이터-불러오기)
3. [형태소 분석기 설정](#3-형태소-분석기-설정)
4. [복합명사 등록](#4-복합명사-등록)
5. [DTM 생성](#5-dtm-생성)
6. [토픽모델링](#6-토픽모델링)

---

## AI 프롬프트 작성 원칙

### 핵심 원칙

- **구체성**: 변수명, 데이터 구조, 작업 결과를 명시적으로 기술
- **맥락 유지**: 중요한 학습 파일과 예시코드 반복 제공
- **프로세스 명확화**: 예정된 작업 순서를 사전에 설명
- **범위 제한**: 과도한 확장보다는 집중된 작업 지시

### 주의사항

- AI는 이전 맥락을 완전히 기억하지 못함
- 중요한 매뉴얼과 예시코드는 반복 학습 필요
- 명확한 프로세스 설명으로 작업 방향성 제시
- AI의 맥락 처리 한계를 고려한 단계적 접근

---

## 1. 환경 셋팅하기

**입력 프롬프트**:

```
/init 현재 프로젝트, 윈도우 환경, vs code 에서 r. r 스크립트를 실행하면서 발생하는 오류를 파악하고 근본적으로 해결.
```

## 2. 데이터 불러오기

**입력 프롬프트**:

```
data\raw_data 폴더에서 데이터 불러오기, 여러 파일이 있으면 병합하고 데이터 구조 보고서 md 파일로 작성. 보고서 폴더 만들고 . 스크립트 이름에 순서 적용. 예를 들어 01.~
```

- 폴더 구조화, 스크립트 세분화 등은 프롬프트 입력 결과에 따라 세부조정.
  ex) 데이터 불러오기와, 데이터 보고에 대한 스크립트가 분리되었다면 통합하게 한다거나. 반대의 경우 분리하게 한다거나.
- 일관된 작업이 필요하면 프로젝트 지침으로 입력하기

## 3. 형태소 분석기 설정

### 3.1 형태소 분석기 학습시키기 (개선된 프롬프트)

**입력 프롬프트**:

```
https://github.com/bab2min/Kiwi
https://github.com/bab2min/kiwipiepy
이 사이트를 참조하여 python 형태소 분석기를 R에서 사용.

**핵심 요구사항:**
1. 분석 대상: '초록' 컬럼, '논문 ID'는 doc_id로 보존
2. 결과 데이터 구조:
   - doc_id: 원본 데이터 매칭용 ID
   - morpheme_analysis: 전체 형태소 분석 결과 ("문장/NNG", "을/JKO", "입력하/VV", "ㅂ니다/EF", "./SF")
   - noun_extraction: 명사만 추출하여 쉼표로 구분 ("학습, 모델, 분석")
   
3. **XSN 파생접미사 처리 (중요):**
   - 선행명사+파생접미사(XSN) 자동 결합
   - 예: "학습/NNG + 자/XSN" → "학습자/NNG"로 통합
   - XSN 태그 패턴 자동 감지하여 처리

4. 명사 태그: NNG(일반명사), NNP(고유명사) 추출
5. 사용자 사전 적용 옵션: 대화형으로 선택 가능
6. 파일럿 테스트: 처음 10개 문서로 테스트 후 전체 적용 선택
7. 결과 저장: RDS, CSV 형태로 저장
8. 진행상황 표시: 대용량 데이터 처리 시 진행률 표시

**에러 처리:**
- kiwipiepy 설치 오류 시 자동 설치 시도
- Python 환경 문제 시 해결 가이드 제공
- 메모리 부족 시 배치 처리 옵션

**출력:**
- 형태소 분석 통계 보고서
- XSN 패턴 처리 현황 보고
- 명사 추출 결과 샘플 확인
```

### 3.2 형태소 분석기 점검

- 오류 메세지 프롬프트 창에 입력하기.
- 결과 파일 보면서 디테일 수정하기.

## 4. 복합명사 등록

### 4.1 N그램 분석 (개선된 프롬프트)

**입력 프롬프트**:

```
02번 스크립트의 명사 추출 결과를 사용하여 N그램 분석 수행.

**핵심 요구사항:**
1. **대화형 설정:**
   - N그램 크기 선택 (2-5그램, 기본값: 2,3)
   - 최소 빈도 임계값 설정 (기본값: 2회)
   - 최대 결과 개수 설정 (기본값: 100개)

2. **복합명사 후보 필터링:**
   - 최소 2단어 이상 조합
   - 각 구성 단어 최소 1글자 이상
   - 전체 길이 20글자 이하
   - 한글만 포함 (영문, 숫자, 특수문자 제외)

3. **결과 파일 생성:**
   - `compound_nouns_candidates_YYYYMMDD.csv`: 복합명사 후보
     * 컬럼: ngram, frequency, ngram_size, pos_tag
   - `proper_nouns_candidates_YYYYMMDD.csv`: 고유명사 빈 템플릿
     * 컬럼: noun, pos_tag
   
4. **분석 보고서:**
   - 상위 빈도 N그램 리스트
   - N그램 크기별 분포
   - 빈도 그래프 및 워드클라우드 (선택사항)

5. **사용자 검토 가이드:**
   - CSV 파일 검토 방법 안내
   - 불필요한 행 삭제 가이드
   - 다음 단계 안내
```

### 4.2 복합명사 검토

- 사용자가 추출된 N 그램 리스트를 검토하면서 등록단어와 미등록단어를 구분

### 4.3 고유명사 사전 구축

**입력 프롬프트**:

```
사용자가 고유명사 사전을 등록할수 있도록 비어있는 고유명사 CSV 파일 생성. 사용자에 의해 검토된 CSV 파일 역시 사용자 검토후 사전 구성 예정.
```

### 4.4 사전 생성과 분석기 적용 (개선된 프롬프트)

**입력 프롬프트**:

```
사용자가 검토 완료한 CSV 파일들을 Kiwipiepy 사용자 사전으로 변환.

**핵심 요구사항:**
1. **다중 파일 지원:**
   - 복합명사 후보 파일 다중 선택 가능
   - 고유명사 후보 파일 다중 선택 가능
   - 최신 파일 자동 선택 옵션

2. **기존 사전 병합:**
   - 기존 사전 파일 감지
   - 병합 vs 신규 생성 선택
   - 중복 단어 자동 제거

3. **사전 이름 설정:**
   - 기본 이름: kiwi_user_dict_YYYYMMDD
   - 사용자 정의 이름 입력
   - 용도별 템플릿 (educational, academic, specialized 등)

4. **Kiwi 형식 준수:**
   - 탭 구분 형식: [단어]\t[품사]\t[점수]
   - 복합명사 띄어쓰기 제거
   - UTF-8 인코딩

5. **품사 태그 매핑:**
   - 복합명사 → NNG
   - 고유명사 → NNP
   - 기본 점수: 0.0

6. **검증 및 미리보기:**
   - 사전 내용 미리보기 (상위 10개)
   - 품사별 통계
   - 파일 크기 및 단어 수 표시

7. **다음 단계 안내:**
   - 02번 스크립트 재실행 가이드
   - 사용자 사전 선택 방법 안내

사전 구성방법은 다음 사이트 참조:
https://github.com/bab2min/Kiwi
https://github.com/bab2min/kiwipiepy
```

### 4.5 프로세스 반복 및 개선

**입력 프롬프트**:

```
사용자 사전을 적용한 후 형태소 분석을 다시 실행하여 개선 효과를 확인하고, 필요시 N그램 분석을 반복하여 추가 복합명사를 발견하는 반복적 개선 프로세스를 구현해줘. 

**핵심 요구사항:**
1. **이전 분석과 비교:**
   - 사전 적용 전후 명사 추출 결과 비교
   - 새로 인식된 복합명사 통계
   - 개선 효과 정량적 측정

2. **새로운 후보 발견:**
   - 추가 복합명사 후보 제시
   - 이전에 놓친 패턴 탐지
   - 빈도 변화 추적

3. **반복 관리:**
   - 반복 횟수 추적 및 수렴 조건 설정
   - 개선 효과 임계값 설정
   - 자동 종료 조건

4. **진행상황 보고:**
   - 각 라운드별 개선 사항 보고
   - 사전 크기 변화 추적
   - 최종 개선 효과 요약
```

### ※ 작업 프로세스 요약

**현재까지 작업된 프로세스**:
데이터 임포트 → 형태소 분석 → n그램 분석 → 복합명사, 고유명사 예비 단어 선정, 등록 → 사용자 사전 생성 → 생성된 사용자 사전으로 형태소 재분석

### ☆ *이 과정을 반복하면서 코퍼스 정제*

## 5. DTM 생성

- 명사 추출된 열과 문서를 기준으로 dtm 생성. 여러 방식이 있으니 여러 방식 설명을 요구하고 맥락을 만든후에 dtm 생성 지시.
- 데이터 저장 형태를 명시. 원본데이터와 형태소 분석 명사추출 데이터를 명시하면 오류 줄어듬.
- 자료 저장 형태 명시 하면 오류 줄어듬.
- 오류를 확인하면서 세부조정.
- 필요하면 보고서

**입력 프롬프트**:

```
명사 추출된 데이터 열을 논문 id를 참조하여 최초 임포트한 원본 데이터의 메타데이터를 보존하는 방식으로 dtm을 구성해줘. quanteda 사용. 명사추출 결과 명사는 ','로 구분되어 저장되어 있음.
```

### 5.1 빈도분석, TF-IDF 분석

```
DTM을 이용해 빈도분석, tf-idf 분석을 상위 20개 정렬.
```

### 5.2 희소성 처리, DTM 구성에서 tf-idf 적용여부 선택

**입력 프롬프트**:

```
DTM 처리시 최소용어빈도, 최소 문서 빈도를 설정하여 희소성 관리할수 있도록 스크립트 구성. 적용 전후 희소성 변화 여부 파악. 
```

  📊 전체 토픽모델링 방법 호환성 매트릭스

| 토픽모델링 방법 | TF-IDF 호환성 | 권장 DTM   | R 패키지    | 특징                 |
| --------------- | ------------- | ---------- | ----------- | -------------------- |
| LDA             | ❌ 비권장     | 원본 DTM   | topicmodels | 기본 확률적 토픽모델 |
| STM             | ⚠️ 조건부   | 원본 DTM   | stm         | 메타데이터 활용 LDA  |
| BTM             | ❌ 비권장     | 원본 DTM   | BTM         | 짧은 텍스트 특화     |
| CTM             | ❌ 비권장     | 원본 DTM   | topicmodels | 토픽간 상관관계      |
| NMF             | ✅ 권장       | TF-IDF DTM | NMF         | 비음수 행렬분해      |
| LSI/LSA         | ✅ 권장       | TF-IDF DTM | RSpectra    |                      |

### 5.3 동의어 처리, 한자어, 영문, 숫자 처리 (개선된 프롬프트)

**동의어 처리 프롬프트**:

```
DTM 생성 후 동의어 처리를 통한 용어 통합 구현.

**핵심 요구사항:**
1. **동의어 사전 형식:**
   - CSV 파일: 첫 번째 열(대표어), 2,3,4...열(동의어들)
   - 가변 길이 컬럼 지원
   - UTF-8 인코딩

2. **처리 방식:**
   - 모든 동의어를 대표어로 통합
   - DTM에서 해당 feature 자동 병합
   - 희소성 개선 효과 측정

3. **사용자 인터페이스:**
   - 동의어 사전 파일 선택 옵션
   - 적용 전후 통계 비교
   - 처리 결과 미리보기
```

**한자어/영문/숫자 제거 프롬프트**:

```
DTM에서 한자어, 영문, 숫자가 포함된 용어를 자동 제거.

**핵심 요구사항:**
1. **자동 패턴 인식:**
   - 한자어, 영문, 숫자 패턴

2. **처리 옵션:**
   - 각 패턴별 개별 제거 선택
   - 제거 전후 용어 수 비교
   - 제거된 용어 샘플 표시

3. **결과 보고:**
   - 제거 통계 (개수, 비율)
   - DTM 희소성 변화
   - 최종 용어 목록 정제 효과
```

## 6. 토픽모델링

### ※ 모델 설계 원칙

- 단일 모델 스크립트부터 작성하여 확장
- 처음부터 복잡하게 설계하면 디버깅이 어려움
- STM의 파라미터와 공변량 설정에 주의

### 6.2 STM 분석 파라미터 설정

**입력 프롬프트**:

```
stm 토픽모델링. quanteda를 사용하여 메타데이터가 보존된 dtm을 stm 입력데이터로 변형. 
K값은 8. 공변량은 메타데이터의 '발행연도'를 사용. 스플라인 함수 적용. 
'발행연도' 데이터 형식을 검토하고 문자형일 때, 숫자형이나 날짜형으로 변환. 

```

### 6.3 모델 확장 전략

- 세부 디버깅을 통한 조정
- 모델 한 개 성공 후 나머지 프롬프트로 지시하여 완료
- K값 비교를 위한 스크립트 작성. 등등

### 6.4 STM Prevalence 매개변수 가이드

- 초기 모델 학습이 성공하면 Prevalence 매개변수 설정을 위한 코드 수정을 요구할 수 있음.

#### 6.4.1 Prevalence 기본 개념

**Prevalence**는 각 토픽이 문서에서 출현할 확률을 문서의 메타데이터(공변량)에 따라 조절하는 기능.

- **목적**: "토픽의 인기도가 시간/저자/카테고리에 따라 어떻게 변하는가?" 분석
- **효과**: 토픽별 출현 패턴의 시간적/범주적 변화 탐지
- **활용**: 연구 트렌드, 저자별 관심사, 기관별 특성 등 분석

#### 6.4.2 함수별 사용법

**1. 선형 함수 (Linear)**

```r
prevalence = ~ year_processed
# 연도에 따른 선형적 변화
```

**2. 스플라인 함수 (Smooth/Non-linear)**

```r
prevalence = ~ s(year_processed)
# 연도에 따른 비선형적 변화 (곡선)
prevalence = ~ s(year_processed, k=5)  # 매끄러움 조절
```

**3. 팩터 변수 (Categorical)**

```r
prevalence = ~ factor(author_type)
# 저자 유형별 차이 (개인/기관/공동연구 등)
prevalence = ~ as.factor(journal_category)
# 저널 카테고리별 차이
```

**4. 다항식 함수 (Polynomial)**

```r
prevalence = ~ poly(year_processed, 2)
# 2차 곡선 (포물선)
prevalence = ~ poly(year_processed, 3)
# 3차 곡선
```

#### 6.4.3 다중 변수 조합

**1. 가산 모델 (Additive)**

```r
prevalence = ~ s(year_processed) + factor(journal_type)
# 연도 트렌드 + 저널 유형별 차이
```

**2. 상호작용 효과 (Interaction)**

```r
prevalence = ~ s(year_processed) * factor(research_field)
# 연구 분야별로 다른 시간 트렌드
```

**3. 조건부 모델**

```r
prevalence = ~ factor(institution_type) + factor(institution_type):s(year_processed)
# 기관 유형별로 다른 시간 변화
```

#### 6.4.4 실제 사용 예시

**연구 트렌드 분석**

```r
# 교육학 연구 주제의 시대별 변화
stm_model <- stm(documents = docs, 
                 vocab = vocab,
                 K = 8,
                 prevalence = ~ s(year_processed),
                 data = meta)
```

**저자별 특성화 분석**

```r
# 개인 vs 공동연구자의 토픽 선호도
prevalence = ~ factor(collaboration_type) + s(year_processed)
```

**기관별 연구 특성**

```r
# 대학 vs 연구소의 연구 주제 차이
prevalence = ~ factor(institution_type) * s(year_processed)
```

#### 6.4.5 결과 해석 및 시각화

**1. 토픽별 트렌드 시각화**

```r
# 토픽별 시간 변화 그래프
plot(stm_model, type="perspectives", topics=c(1,2,3))

# 특정 토픽의 시간별 변화
plot(stm_model, type="hist")
```

**2. 공변량 효과 분석**

```r
# 공변량별 토픽 확률 비교
plot(stm_model, type="summary")

# 예상 토픽 비율 계산
estimateEffect(1:K ~ s(year_processed), stm_model, meta)
```

**3. 해석 가이드라인**

- **상승 트렌드**: 해당 주제의 관심도 증가
- **하락 트렌드**: 전통적 주제의 쇠퇴
- **변곡점**: 패러다임 전환점 또는 중요 사건
- **그룹 차이**: 집단별 연구 관심사 차이

#### 6.4.6 고급 활용 전략

- **단계별 접근**: 단일 공변량 → 다중 공변량 → 상호작용
- **모델 비교**: AIC/BIC를 통한 최적 모델 선택
- **K값 최적화**: 여러 K값 비교를 통한 최적 토픽 수 결정
- **세부 디버깅**: 수렴성, 해석가능성, 통계적 유의성 검토

### 6.5 STM 결과 출력 및 시각화

#### 6.5.1 기본 결과 출력

**입력 프롬프트**: `STM 모델의 기본 정보와 토픽별 키워드를 출력하는 스크립트 작성. summary(), print(), labelTopics() 함수 활용하여 모델 성능과 각 토픽의 상위 10개 단어 출력.`

#### 6.5.2 시각화 및 그래프

**입력 프롬프트**: `STM 결과를 다양한 형태로 시각화. plot() 함수의 type 옵션을 활용하여 토픽 요약차트(summary), 토픽 분포 히스토그램(hist), 토픽간 상관관계(perspectives), 워드클라우드(cloud) 생성.`

- `type="summary"`: 토픽별 상위 단어와 비율 차트
- `type="hist"`: 문서별 토픽 분포 히스토그램
- `type="perspectives"`: 두 토픽 간 차이점 비교
- `type="cloud"`: 특정 토픽의 워드클라우드

#### 6.5.3 모델 비교 및 최적화

**입력 프롬프트**: `여러 K값으로 학습된 STM 모델들의 성능을 비교하는 스크립트 작성. searchK() 함수로 K값별 모델 평가지표 계산하고, plotModels()로 시각화. exclusivity와 semantic coherence 지표 포함.`

#### 6.5.4 시간 변화 분석

**입력 프롬프트**: `prevalence 공변량이 설정된 STM 모델에서 토픽별 시간 변화를 분석. estimateEffect() 함수로 공변량 효과 계산하고, plotTopicLoess()로 시간에 따른 토픽 비율 변화 시각화.`

#### 6.5.5 대표 문서 추출

**입력 프롬프트**: `각 토픽을 가장 잘 대표하는 문서들을 findThoughts() 함수로 추출. 토픽별로 상위 3개 대표문서 텍스트 출력하고, plotQuote()로 인용구 형태로 시각화.`

#### 6.5.6 데이터 추출 및 저장

**입력 프롬프트**: `STM 모델의 결과 데이터를 추출하여 CSV 파일로 저장. theta(문서-토픽 확률), beta(토픽-단어 확률) 행렬 추출하고, tidytext 패키지의 tidy() 함수로 정리된 형태로 변환하여 저장.`

#### 6.5.7 종합 보고서 자동 생성

**입력 프롬프트**: `STM 분석의 모든 결과를 포함한 종합 보고서를 자동 생성하는 함수 작성. 모델 요약, 토픽별 키워드, 대표문서, 시각화 차트를 순서대로 출력하는 stm_report() 함수 구현.`

- 모델 기본 정보 (K값, 문서수, 어휘수)
- 토픽별 상위 키워드 리스트
- 토픽 요약 시각화
- 각 토픽의 대표 문서 3개씩 출력
- 결과 파일 자동 저장 (CSV, PNG)
