# 한국어 형태소 분석 프롬프트 가이드

## 📋 목차

0. [AI 프롬프트 작성 원칙](#ai-프롬프트-작성-원칙)
1. [환경 셋팅하기](#1-환경-셋팅하기)
2. [데이터 불러오기](#2-데이터-불러오기)
3. [형태소 분석기 설정](#3-형태소-분석기-설정)
4. [복합명사 등록](#4-복합명사-등록)
5. [DTM 생성](#5-dtm-생성)
6. [토픽모델링](#6-토픽모델링)

---

## AI 프롬프트 작성 원칙

### 핵심 원칙

- **구체성**: 변수명, 데이터 구조, 작업 결과를 명시적으로 기술
- **맥락 유지**: 중요한 학습 파일과 예시코드 반복 제공
- **프로세스 명확화**: 예정된 작업 순서를 사전에 설명
- **범위 제한**: 과도한 확장보다는 집중된 작업 지시

### 주의사항

- AI는 이전 맥락을 완전히 기억하지 못함
- 중요한 매뉴얼과 예시코드는 반복 학습 필요
- 명확한 프로세스 설명으로 작업 방향성 제시
- AI의 맥락 처리 한계를 고려한 단계적 접근

---

## 1. 환경 셋팅하기

**입력 프롬프트**: `/init 현재 프로젝트, 윈도우 환경, vs code 에서 r.` r 스크립틀르 실행하면서 발생하는 오류를 파악하고 근본적으로 해결.

## 2. 데이터 불러오기

**입력 프롬프트**: `data\raw_data 폴더에서 데이터 불러오기, 여러 파일이 있으면 병합하고 데이터 구조 보고서 md 파일로 작성. 보고서 폴더 만들고 . 스크립트 이름에 순서 적용. 예를 들어 01.~`

- 폴더 구조화, 스크립트 세분화 등은 프롬프트 입력 결과에 따라 세부조정.
  ex) 데이터 불러오기와, 데이터 보고에 대한 스크립트가 분리되었다면 통합하게 한다거나. 반대의 경우 분리하게 한다거나.
- 일관된 작업이 필요하면 프로젝트 지침으로 입력하기

## 3. 형태소 분석기 설정

### 3.1 형태소 분석기 학습시키기

**입력 프롬프트**:

```
https://github.com/bareun-nlp/RBareun
https://github.com/bareun-nlp/RBareun/blob/main/samples/custom_dict.R
https://github.com/bareun-nlp/RBareun/tree/main/R
형태소 분석기 사용법 학습해서 예시 스크립트 작성. 분석 텍스트는 '초록'. '논문 id'는 원 데이터와 매칭을 위해 보존.
결과 데이터 구조는 doc_id, 형태소 분석("문장/NNG"  "을/JKO"    "입력하/VV" "ㅂ니다/EF" "./SF"), 명사 추출(태깅 NNP, NNG)
명사 추출은 명사와 명사를 ','로 구분하여 한개로 생성.명사+파생접미사(XSN) 자동 결합후 한개의 명사로 추출. 테스트를 위해 10개 문서만 추출해서 파일럿 분석.
```

### 3.2 형태소 분석기 점검

- 입력 프롬프트에 API_KEY 정보와 서버주소 입력. !!주의 api는 하드코딩하지 않도록 환경변수 설정. 서버주소는 포트없이 'localhost'

**입력 프롬프트**: `api 는 환경변수로 설정하였으니 참고(또는 xx 파일에 저장). 서버 주소는 'localhost'로 입력`

### 3.3 사용자 정의사전 설정 반영

- 사용자 정의 사전 설정은 후속 프로세스에 의존하므로 이를 스크립트에 반영하도록 유도

**입력 프롬프트**: `현재 사용자 사전을 조회하고 그 결과에 의해 사용자 사전 적용과 미적용 옵션을 선택할 수 있도록 형태소 분석기를 구성.`

### 3.4 추가 디버깅

- 오류 메세지 프롬프트 창에 입력하기.
- 결과 파일 보면서 디테일 수정하기.

## 4. 복합명사 등록

### 4.1 N 그램 분석

- N그램 분석을 통해 복합명사 후보를 생성

**입력 프롬프트**: `N 그램 분석 스크립트 작성. N그램 분석의 결과는 사용자가 예비 등록 단어로 검토할수 있도록 CSV 파일저장. 분석결과는 보고서 작성.`

### 4.2 복합명사 검토

- 사용자가 추출된 N 그램 리스트를 검토하면서 등록단어와 미등록단어를 구분

### 4.3 복합명사 사전 구축

**입력 프롬프트**: `사용자가 검토한 CSV를 참조하여 형태소 분석기 사전 예제와 예시코드를 아래 깃헙 페이지를 참조하여 사전을 구성하고 등록`

```
https://github.com/bareun-nlp/RBareun
https://github.com/bareun-nlp/RBareun/blob/main/samples/custom_dict.R
https://github.com/bareun-nlp/RBareun/tree/main/R
```

### 4.4 고유명사 사전 구축

**입력 프롬프트**: `사용자가 고유명사 사전을 등록할수 있도록 비어있는 고유명사 CSV 파일 생성. 사용자에 의해 검토된 CSV 파일 역시 사용자 사전에 등록할 수 있도록 코드 수정. 복합명사와 고유명사 사전을 통합한 통합사전 구축. 복학명사의 경우 등록 단어의 띄어쓰기 상태 주의.`

### 4.5 작업 프로세스 요약

**현재까지 작업된 프로세스**:
데이터 임포트 → 형태소 분석 → n그램 분석 → 복합명사, 고유명사 예비 단어 선정, 등록 → 사용자 사전 생성 → 생성된 사용자 사전으로 형태소 재분석

*이 과정을 반복하면서 코퍼스 정제*

## 5. DTM 생성

- 명사 추출된 열과 문서를 기준으로 dtm 생성. 여러 방식이 있으니 여러 방식 설명을 요구하고 맥락을 만든후에 dtm 생성 지시.
- 데이터 저장 형태를 명시. 원본데이터와 형태소 분석 명사추출 데이터를 명시하면 오류 줄어듬.
- 자료 저장 형태 명시 하면 오류 줄어듬.
- 오류를 확인하면서 세부조정.
- 필요하면 보고서

**입력 프롬프트**: 명사 추출된 데이터 열을 논문 id를 참조하여 최초 임포트한 원본 데이터의 메타데이터를 보존하는 방식으로 dtm을 구성해줘. quanteda 사용. 명사추출 결과 명사는 ','로 구분되어 저장되어 있음.

### 5.1 빈도분석, TF-IDF 분석

**입력 프롬프트**: DTM을 이용해 빈도분석, tf-idf 분석을 상위 20개 정렬.

### 5.2 희소성 처리, tf-idf 적용여부 선택

최소 용어 빈도, 최소 문서 빈도 설정으로 희소성 관리. tf-idf 적용여부 dtm 생성에 반영.

  📊 전체 토픽모델링 방법 호환성 매트릭스

| 토픽모델링 방법 | TF-IDF 호환성 | 권장 DTM   | R 패키지    | 특징                 |
| --------------- | ------------- | ---------- | ----------- | -------------------- |
| LDA             | ❌ 비권장     | 원본 DTM   | topicmodels | 기본 확률적 토픽모델 |
| STM             | ⚠️ 조건부   | 원본 DTM   | stm         | 메타데이터 활용 LDA  |
| BTM             | ❌ 비권장     | 원본 DTM   | BTM         | 짧은 텍스트 특화     |
| CTM             | ❌ 비권장     | 원본 DTM   | topicmodels | 토픽간 상관관계      |
| NMF             | ✅ 권장       | TF-IDF DTM | NMF         | 비음수 행렬분해      |
| LSI/LSA         | ✅ 권장       | TF-IDF DTM | RSpectra    |                      |

### 5.3 동의어 처리, 한자어, 영문, 숫자 처리

**입력 프롬프트**: dtm 생성이후 동의어 처리를 위한 작업을 하려고해 작업 방식은 csv 파일의 2,3,4,....열 이후 의 단어를 첫번째 열의 단어로 동의어 처리하는 방식.

**입력 프롬프트**: 한자어, 영문, 숫자를 dtm 에서 제거.

## 6. 토픽모델링

### 6.1 모델 설계 원칙

- 단일 모델 스크립트부터 작성하여 확장
- 처음부터 복잡하게 설계하면 디버깅이 어려움
- STM의 파라미터와 공변량 설정에 주의

### 6.2 STM 분석 파라미터 설정

**입력 프롬프트**:

```
stm 토픽모델링. quanteda를 사용하여 메타데이터가 보존된 dtm을 stm 입력데이터로 변형. 
K값은 8. 공변량은 메타데이터의 '발행연도'를 사용. 스플라인 함수 적용. 
'발행연도' 데이터 형식을 검토하고 문자형일 때, 숫자형이나 날짜형으로 변환. 
토픽-단어 할당만 모델링.
```

### 6.3 모델 확장 전략

- 세부 디버깅을 통한 조정
- 모델 한 개 성공 후 나머지 프롬프트로 지시하여 완료
- K값 비교를 위한 스크립트 작성. 등등

### 6.4 STM Prevalence 매개변수 가이드

#### 6.4.1 Prevalence 기본 개념

**Prevalence**는 각 토픽이 문서에서 출현할 확률을 문서의 메타데이터(공변량)에 따라 조절하는 기능입니다.

- **목적**: "토픽의 인기도가 시간/저자/카테고리에 따라 어떻게 변하는가?" 분석
- **효과**: 토픽별 출현 패턴의 시간적/범주적 변화 탐지
- **활용**: 연구 트렌드, 저자별 관심사, 기관별 특성 등 분석

#### 6.4.2 함수별 사용법

**1. 선형 함수 (Linear)**
```r
prevalence = ~ year_processed
# 연도에 따른 선형적 변화
```

**2. 스플라인 함수 (Smooth/Non-linear)**
```r
prevalence = ~ s(year_processed)
# 연도에 따른 비선형적 변화 (곡선)
prevalence = ~ s(year_processed, k=5)  # 매끄러움 조절
```

**3. 팩터 변수 (Categorical)**
```r
prevalence = ~ factor(author_type)
# 저자 유형별 차이 (개인/기관/공동연구 등)
prevalence = ~ as.factor(journal_category)
# 저널 카테고리별 차이
```

**4. 다항식 함수 (Polynomial)**
```r
prevalence = ~ poly(year_processed, 2)
# 2차 곡선 (포물선)
prevalence = ~ poly(year_processed, 3)
# 3차 곡선
```

#### 6.4.3 다중 변수 조합

**1. 가산 모델 (Additive)**
```r
prevalence = ~ s(year_processed) + factor(journal_type)
# 연도 트렌드 + 저널 유형별 차이
```

**2. 상호작용 효과 (Interaction)**
```r
prevalence = ~ s(year_processed) * factor(research_field)
# 연구 분야별로 다른 시간 트렌드
```

**3. 조건부 모델**
```r
prevalence = ~ factor(institution_type) + factor(institution_type):s(year_processed)
# 기관 유형별로 다른 시간 변화
```

#### 6.4.4 실제 사용 예시

**연구 트렌드 분석**
```r
# 교육학 연구 주제의 시대별 변화
stm_model <- stm(documents = docs, 
                 vocab = vocab,
                 K = 8,
                 prevalence = ~ s(year_processed),
                 data = meta)
```

**저자별 특성화 분석**
```r
# 개인 vs 공동연구자의 토픽 선호도
prevalence = ~ factor(collaboration_type) + s(year_processed)
```

**기관별 연구 특성**
```r
# 대학 vs 연구소의 연구 주제 차이
prevalence = ~ factor(institution_type) * s(year_processed)
```

#### 6.4.5 결과 해석 및 시각화

**1. 토픽별 트렌드 시각화**
```r
# 토픽별 시간 변화 그래프
plot(stm_model, type="perspectives", topics=c(1,2,3))

# 특정 토픽의 시간별 변화
plot(stm_model, type="hist")
```

**2. 공변량 효과 분석**
```r
# 공변량별 토픽 확률 비교
plot(stm_model, type="summary")

# 예상 토픽 비율 계산
estimateEffect(1:K ~ s(year_processed), stm_model, meta)
```

**3. 해석 가이드라인**
- **상승 트렌드**: 해당 주제의 관심도 증가
- **하락 트렌드**: 전통적 주제의 쇠퇴
- **변곡점**: 패러다임 전환점 또는 중요 사건
- **그룹 차이**: 집단별 연구 관심사 차이

#### 6.4.6 고급 활용 전략

- **단계별 접근**: 단일 공변량 → 다중 공변량 → 상호작용
- **모델 비교**: AIC/BIC를 통한 최적 모델 선택  
- **K값 최적화**: 여러 K값 비교를 통한 최적 토픽 수 결정
- **세부 디버깅**: 수렴성, 해석가능성, 통계적 유의성 검토

### 6.5 STM 결과 출력 및 시각화

#### 6.5.1 기본 결과 출력

**입력 프롬프트**: `STM 모델의 기본 정보와 토픽별 키워드를 출력하는 스크립트 작성. summary(), print(), labelTopics() 함수 활용하여 모델 성능과 각 토픽의 상위 10개 단어 출력.`

#### 6.5.2 시각화 및 그래프

**입력 프롬프트**: `STM 결과를 다양한 형태로 시각화. plot() 함수의 type 옵션을 활용하여 토픽 요약차트(summary), 토픽 분포 히스토그램(hist), 토픽간 상관관계(perspectives), 워드클라우드(cloud) 생성.`

- `type="summary"`: 토픽별 상위 단어와 비율 차트
- `type="hist"`: 문서별 토픽 분포 히스토그램
- `type="perspectives"`: 두 토픽 간 차이점 비교
- `type="cloud"`: 특정 토픽의 워드클라우드

#### 6.5.3 모델 비교 및 최적화

**입력 프롬프트**: `여러 K값으로 학습된 STM 모델들의 성능을 비교하는 스크립트 작성. searchK() 함수로 K값별 모델 평가지표 계산하고, plotModels()로 시각화. exclusivity와 semantic coherence 지표 포함.`

#### 6.5.4 시간 변화 분석

**입력 프롬프트**: `prevalence 공변량이 설정된 STM 모델에서 토픽별 시간 변화를 분석. estimateEffect() 함수로 공변량 효과 계산하고, plotTopicLoess()로 시간에 따른 토픽 비율 변화 시각화.`

#### 6.5.5 대표 문서 추출

**입력 프롬프트**: `각 토픽을 가장 잘 대표하는 문서들을 findThoughts() 함수로 추출. 토픽별로 상위 3개 대표문서 텍스트 출력하고, plotQuote()로 인용구 형태로 시각화.`

#### 6.5.6 데이터 추출 및 저장

**입력 프롬프트**: `STM 모델의 결과 데이터를 추출하여 CSV 파일로 저장. theta(문서-토픽 확률), beta(토픽-단어 확률) 행렬 추출하고, tidytext 패키지의 tidy() 함수로 정리된 형태로 변환하여 저장.`

#### 6.5.7 종합 보고서 자동 생성

**입력 프롬프트**: `STM 분석의 모든 결과를 포함한 종합 보고서를 자동 생성하는 함수 작성. 모델 요약, 토픽별 키워드, 대표문서, 시각화 차트를 순서대로 출력하는 stm_report() 함수 구현.`

- 모델 기본 정보 (K값, 문서수, 어휘수)
- 토픽별 상위 키워드 리스트
- 토픽 요약 시각화
- 각 토픽의 대표 문서 3개씩 출력
- 결과 파일 자동 저장 (CSV, PNG)
